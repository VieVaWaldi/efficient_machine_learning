submit host:
fj081
submit dir:
/lustre/home/wehrenberger/code/efficient_machine_learning/8th_chapter
nodelist:
fj-epyc
##############################################
# Welcome to EML's U-Net for seismic example #
##############################################
CUDA devices: 2
   Tesla V100-PCIE-32GB
   Tesla V100-PCIE-32GB
printing configuration:
{
  "unet": {
    "n_init_channels": 60,
    "kernel_size": 3,
    "n_layers_per_block": 2,
    "n_levels": 3
  },
  "train": {
    "data": {
      "seismic": "data/data_train.npz",
      "labels": "data/labels_train.npz",
      "sample_shape": [
        1004,
        1,
        588
      ],
      "subset": [
        [
          0,
          1004
        ],
        [
          0,
          750
        ],
        [
          0,
          588
        ]
      ]
    },
    "n_epochs": 50,
    "n_epochs_print": 20,
    "n_batch_abort": 5000,
    "batch_size": 8
  },
  "test": {
    "data": {
      "seismic": "data/data_train.npz",
      "labels": "data/labels_train.npz",
      "sample_shape": [
        1004,
        1,
        588
      ],
      "subset": [
        [
          0,
          1004
        ],
        [
          751,
          782
        ],
        [
          0,
          588
        ]
      ]
    },
    "batch_size": 1
  }
}
********************
* assembling U-Net *
********************
encoder:
Sequential(
  (0): Conv2d(1, 60, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1))
  (4): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
)
Sequential(
  (0): Conv2d(60, 120, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1))
  (4): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
)
max_pooling:
MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
crop:
ZeroPad2d((-16, -16, -16, -16))
ZeroPad2d((-4, -4, -4, -4))
bottleneck:
Sequential(
  (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1))
  (4): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
)
up_sampling:
Upsample(scale_factor=2.0, mode=bilinear)
decoder:
Sequential(
  (0): Conv2d(120, 60, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1))
  (4): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
)
Sequential(
  (0): Conv2d(240, 60, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1))
  (4): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
)
classification:
Conv2d(60, 8, kernel_size=(1, 1), stride=(1, 1))
*****************
* prepping data *
*****************
loading training dataset
shape padded: (1004, 750, 588)
shape: (1, 750, 1)
loading test data set
shape padded: (1004, 31, 588)
shape: (1, 31, 1)
deriving mean and standard deviation of training data
  mean: 0.6930369
  std: 388.38947
normalizing training and test data
initializing data loaders
************
* training *
************
training epoch 1
  processed 94 batches
  training loss: 121.85183537006378
  new learning rate: 0.001
applying net to test data
  test loss: 32.80365741252899
  test accuracy: 0.59901772254176
training epoch 2
  processed 94 batches
  training loss: 79.52774620056152
  new learning rate: 0.0005
applying net to test data
  test loss: 27.85988974571228
  test accuracy: 0.6586303414565517
training epoch 3
  processed 94 batches
  training loss: 63.836851954460144
  new learning rate: 0.0003333333333333333
applying net to test data
  test loss: 26.493073761463165
  test accuracy: 0.6750234116930965
training epoch 4
  processed 94 batches
  training loss: 55.71537548303604
  new learning rate: 0.00025
applying net to test data
  test loss: 24.566740930080414
  test accuracy: 0.7047265240682463
training epoch 5
  processed 94 batches
  training loss: 50.24444925785065
  new learning rate: 0.0002
applying net to test data
  test loss: 24.220865845680237
  test accuracy: 0.7019616971511254
training epoch 6
  processed 94 batches
  training loss: 46.2314949631691
  new learning rate: 0.00016666666666666666
applying net to test data
  test loss: 23.717543840408325
  test accuracy: 0.7132178120362238
training epoch 7
  processed 94 batches
  training loss: 42.925741136074066
  new learning rate: 0.00014285714285714287
applying net to test data
  test loss: 24.556296229362488
  test accuracy: 0.7025048557585681
training epoch 8
  processed 94 batches
  training loss: 40.333242148160934
  new learning rate: 0.000125
applying net to test data
  test loss: 21.57730346918106
  test accuracy: 0.7342322185931588
training epoch 9
  processed 94 batches
  training loss: 38.40274801850319
  new learning rate: 0.00011111111111111112
applying net to test data
  test loss: 22.377258777618408
  test accuracy: 0.729009286027628
training epoch 10
  processed 94 batches
  training loss: 36.6128466129303
  new learning rate: 0.0001
applying net to test data
  test loss: 20.808520197868347
  test accuracy: 0.7469606322060873
training epoch 11
  processed 94 batches
  training loss: 35.789792627096176
  new learning rate: 9.090909090909092e-05
applying net to test data
  test loss: 21.64007967710495
  test accuracy: 0.7444077562194256
training epoch 12
  processed 94 batches
  training loss: 34.949674755334854
  new learning rate: 8.333333333333333e-05
applying net to test data
  test loss: 21.691109478473663
  test accuracy: 0.7415880333396188
training epoch 13
  processed 94 batches
  training loss: 33.236716002225876
  new learning rate: 7.692307692307693e-05
applying net to test data
  test loss: 20.844788253307343
  test accuracy: 0.7517692498585773
training epoch 14
  processed 94 batches
  training loss: 32.494388192892075
  new learning rate: 7.142857142857143e-05
applying net to test data
  test loss: 24.578924357891083
  test accuracy: 0.7178243710229432
training epoch 15
  processed 94 batches
  training loss: 31.382695645093918
  new learning rate: 6.666666666666667e-05
applying net to test data
  test loss: 22.217146396636963
  test accuracy: 0.7409068715334329
training epoch 16
  processed 94 batches
  training loss: 31.12085446715355
  new learning rate: 6.25e-05
applying net to test data
  test loss: 20.41058909893036
  test accuracy: 0.7632560621263533
training epoch 17
  processed 94 batches
  training loss: 30.131745398044586
  new learning rate: 5.882352941176471e-05
applying net to test data
  test loss: 22.179421067237854
  test accuracy: 0.7445031371913002
training epoch 18
  processed 94 batches
  training loss: 29.897895842790604
  new learning rate: 5.555555555555556e-05
applying net to test data
  test loss: 22.63922870159149
  test accuracy: 0.7432333245727762
training epoch 19
  processed 94 batches
  training loss: 29.47344660758972
  new learning rate: 5.2631578947368424e-05
applying net to test data
  test loss: 20.355506658554077
  test accuracy: 0.7655039266184478
training epoch 20
  processed 94 batches
  training loss: 29.033994048833847
  new learning rate: 5e-05
applying net to test data
  test loss: 20.679351449012756
  test accuracy: 0.7592697847736308
training epoch 21
  processed 94 batches
  training loss: 28.151463121175766
  new learning rate: 4.761904761904762e-05
applying net to test data
  test loss: 22.304734110832214
  test accuracy: 0.7490328174049146
training epoch 22
  processed 94 batches
  training loss: 27.309930860996246
  new learning rate: 4.545454545454546e-05
applying net to test data
  test loss: 23.528124272823334
  test accuracy: 0.7406281172846442
training epoch 23
  processed 94 batches
  training loss: 27.278266042470932
  new learning rate: 4.347826086956522e-05
applying net to test data
  test loss: 20.878329873085022
  test accuracy: 0.763852162668889
training epoch 24
  processed 94 batches
  training loss: 26.893557742238045
  new learning rate: 4.1666666666666665e-05
applying net to test data
  test loss: 21.62970793247223
  test accuracy: 0.7563363008499043
training epoch 25
  processed 94 batches
  training loss: 26.39228230714798
  new learning rate: 4e-05
applying net to test data
  test loss: 20.779349386692047
  test accuracy: 0.7645210507392575
training epoch 26
  processed 94 batches
  training loss: 26.209307819604874
  new learning rate: 3.846153846153846e-05
applying net to test data
  test loss: 21.248886168003082
  test accuracy: 0.7610174792653247
training epoch 27
  processed 94 batches
  training loss: 26.146055042743683
  new learning rate: 3.7037037037037037e-05
applying net to test data
  test loss: 22.178837656974792
  test accuracy: 0.7539186802106832
training epoch 28
  processed 94 batches
  training loss: 25.493724301457405
  new learning rate: 3.571428571428572e-05
applying net to test data
  test loss: 21.442808151245117
  test accuracy: 0.7630062519112832
training epoch 29
  processed 94 batches
  training loss: 25.100984767079353
  new learning rate: 3.4482758620689657e-05
applying net to test data
  test loss: 20.62835341691971
  test accuracy: 0.7698033979562825
training epoch 30
  processed 94 batches
  training loss: 24.950444161891937
  new learning rate: 3.3333333333333335e-05
applying net to test data
  test loss: 20.755474865436554
  test accuracy: 0.7699491562020347
training epoch 31
