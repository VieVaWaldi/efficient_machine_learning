submit host:
login1
submit dir:
/lustre/home/wehrenberger/code/efficient_machine_learning/8th_chapter
nodelist:
fj-epyc
##############################################
# Welcome to EML's U-Net for seismic example #
##############################################
CUDA devices: 2
   Tesla V100-PCIE-32GB
   Tesla V100-PCIE-32GB
printing configuration:
{
  "unet": {
    "n_init_channels": 60,
    "kernel_size": 3,
    "n_layers_per_block": 2,
    "n_levels": 3
  },
  "train": {
    "data": {
      "seismic": "data/data_train.npz",
      "labels": "data/labels_train.npz",
      "sample_shape": [
        1004,
        1,
        588
      ],
      "subset": [
        [
          0,
          1004
        ],
        [
          0,
          750
        ],
        [
          0,
          588
        ]
      ]
    },
    "n_epochs": 50,
    "n_epochs_print": 20,
    "n_batch_abort": 5000,
    "batch_size": 8
  },
  "test": {
    "data": {
      "seismic": "data/data_train.npz",
      "labels": "data/labels_train.npz",
      "sample_shape": [
        1004,
        1,
        588
      ],
      "subset": [
        [
          0,
          1004
        ],
        [
          751,
          782
        ],
        [
          0,
          588
        ]
      ]
    },
    "batch_size": 1
  }
}
********************
* assembling U-Net *
********************
encoder:
Sequential(
  (0): Conv2d(1, 60, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1))
  (4): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
)
Sequential(
  (0): Conv2d(60, 120, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1))
  (4): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
)
max_pooling:
MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
crop:
ZeroPad2d((-16, -16, -16, -16))
ZeroPad2d((-4, -4, -4, -4))
bottleneck:
Sequential(
  (0): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1))
  (4): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
)
up_sampling:
Upsample(scale_factor=2.0, mode=bilinear)
decoder:
Sequential(
  (0): Conv2d(120, 60, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1))
  (4): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
)
Sequential(
  (0): Conv2d(240, 60, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1))
  (4): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
)
classification:
Conv2d(60, 8, kernel_size=(1, 1), stride=(1, 1))
*****************
* prepping data *
*****************
loading training dataset
shape padded: (1004, 750, 588)
shape: (1, 750, 1)
loading test data set
shape padded: (1004, 31, 588)
shape: (1, 31, 1)
deriving mean and standard deviation of training data
  mean: 0.6930369
  std: 388.38947
normalizing training and test data
initializing data loaders
************
* training *
************
training epoch 1
  new learning rate: 0.001
  processed 94 batches
  training loss: 124.7868127822876
applying net to test data
  test loss: 32.92413330078125
  test accuracy: 0.6096523345256158
training epoch 2
  new learning rate: 0.0005
  processed 94 batches
  training loss: 84.05538100004196
applying net to test data
  test loss: 29.00210267305374
  test accuracy: 0.6307769604514586
training epoch 3
  new learning rate: 0.0003333333333333333
  processed 94 batches
  training loss: 72.25696355104446
applying net to test data
  test loss: 26.99647706747055
  test accuracy: 0.6499015170093216
training epoch 4
  new learning rate: 0.00025
  processed 94 batches
  training loss: 66.12791323661804
applying net to test data
  test loss: 26.150646448135376
  test accuracy: 0.659015284892338
training epoch 5
  new learning rate: 0.0002
  processed 94 batches
  training loss: 62.27995443344116
applying net to test data
  test loss: 25.483650505542755
  test accuracy: 0.6707848205274507
training epoch 6
  new learning rate: 0.00016666666666666666
  processed 94 batches
  training loss: 59.32303303480148
applying net to test data
  test loss: 24.50012695789337
  test accuracy: 0.681894566533174
training epoch 7
  new learning rate: 0.00014285714285714287
  processed 94 batches
  training loss: 57.60809361934662
applying net to test data
  test loss: 25.2505304813385
  test accuracy: 0.6720196438393906
training epoch 8
  new learning rate: 0.000125
  processed 94 batches
  training loss: 56.1352264881134
applying net to test data
  test loss: 24.075743198394775
  test accuracy: 0.6876224931047251
training epoch 9
  new learning rate: 0.00011111111111111112
  processed 94 batches
  training loss: 54.59410089254379
applying net to test data
  test loss: 24.181807816028595
  test accuracy: 0.6860358837627146
training epoch 10
  new learning rate: 0.0001
  processed 94 batches
  training loss: 53.39254051446915
applying net to test data
  test loss: 23.91526961326599
  test accuracy: 0.6897133636924087
training epoch 11
  new learning rate: 9.090909090909092e-05
  processed 94 batches
  training loss: 52.43287914991379
applying net to test data
  test loss: 23.720461905002594
  test accuracy: 0.6933348485188959
training epoch 12
  new learning rate: 8.333333333333333e-05
  processed 94 batches
  training loss: 51.73242712020874
applying net to test data
  test loss: 23.344830334186554
  test accuracy: 0.6991155338354533
training epoch 13
  new learning rate: 7.692307692307693e-05
  processed 94 batches
  training loss: 50.8641214966774
applying net to test data
  test loss: 23.000911593437195
  test accuracy: 0.7043220403565319
training epoch 14
  new learning rate: 7.142857142857143e-05
  processed 94 batches
  training loss: 50.14679807424545
applying net to test data
  test loss: 22.857960879802704
  test accuracy: 0.7057439007471225
training epoch 15
  new learning rate: 6.666666666666667e-05
  processed 94 batches
  training loss: 49.530519902706146
applying net to test data
  test loss: 23.338364124298096
  test accuracy: 0.6995282000377128
training epoch 16
  new learning rate: 6.25e-05
  processed 94 batches
  training loss: 49.14447075128555
applying net to test data
  test loss: 22.922143697738647
  test accuracy: 0.7054895107798818
training epoch 17
  new learning rate: 5.882352941176471e-05
  processed 94 batches
  training loss: 48.67039832472801
applying net to test data
  test loss: 23.23525881767273
  test accuracy: 0.7024042844009
training epoch 18
  new learning rate: 5.555555555555556e-05
  processed 94 batches
  training loss: 48.26895472407341
applying net to test data
  test loss: 23.35358887910843
  test accuracy: 0.700378629484127
training epoch 19
  new learning rate: 5.2631578947368424e-05
  processed 94 batches
  training loss: 47.857016146183014
applying net to test data
  test loss: 23.237526655197144
  test accuracy: 0.7019214563953857
training epoch 20
  new learning rate: 5e-05
  processed 94 batches
  training loss: 47.47370111942291
applying net to test data
  test loss: 22.563836634159088
  test accuracy: 0.7114726211423832
training epoch 21
  new learning rate: 4.761904761904762e-05
  processed 94 batches
  training loss: 47.18031916022301
applying net to test data
  test loss: 22.641882240772247
  test accuracy: 0.7098529154580192
training epoch 22
  new learning rate: 4.545454545454546e-05
  processed 94 batches
  training loss: 46.82565540075302
applying net to test data
  test loss: 22.38174843788147
  test accuracy: 0.7147550211181533
training epoch 23
  new learning rate: 4.347826086956522e-05
  processed 94 batches
  training loss: 46.434862703084946
applying net to test data
  test loss: 23.072625637054443
  test accuracy: 0.7052717588300065
training epoch 24
  new learning rate: 4.1666666666666665e-05
  processed 94 batches
  training loss: 46.13469260931015
applying net to test data
  test loss: 22.894151985645294
  test accuracy: 0.7082730841492213
training epoch 25
  new learning rate: 4e-05
  processed 94 batches
  training loss: 45.88425838947296
applying net to test data
  test loss: 22.908084869384766
  test accuracy: 0.7083050202876915
training epoch 26
  new learning rate: 3.846153846153846e-05
  processed 94 batches
  training loss: 45.57026866078377
applying net to test data
  test loss: 22.707140922546387
  test accuracy: 0.7106461895973433
training epoch 27
  new learning rate: 3.7037037037037037e-05
  processed 94 batches
  training loss: 45.44274681806564
applying net to test data
  test loss: 22.859196960926056
  test accuracy: 0.7090877915287042
training epoch 28
  new learning rate: 3.571428571428572e-05
  processed 94 batches
  training loss: 45.37177449464798
applying net to test data
  test loss: 23.025931775569916
  test accuracy: 0.707983582748672
training epoch 29
  new learning rate: 3.4482758620689657e-05
  processed 94 batches
  training loss: 44.868656635284424
applying net to test data
  test loss: 23.142911076545715
  test accuracy: 0.7064941252160422
training epoch 30
  new learning rate: 3.3333333333333335e-05
  processed 94 batches
  training loss: 44.75228437781334
applying net to test data
  test loss: 22.92564618587494
  test accuracy: 0.7088685740581343
training epoch 31
  new learning rate: 3.2258064516129034e-05
  processed 94 batches
  training loss: 44.5324912071228
applying net to test data
  test loss: 22.06043040752411
  test accuracy: 0.7199236683546208
training epoch 32
  new learning rate: 3.125e-05
  processed 94 batches
  training loss: 44.33393093943596
applying net to test data
  test loss: 22.474190831184387
  test accuracy: 0.7146624490609432
training epoch 33
  new learning rate: 3.0303030303030302e-05
  processed 94 batches
  training loss: 44.169389605522156
applying net to test data
  test loss: 22.51787281036377
  test accuracy: 0.714023726291539
training epoch 34
  new learning rate: 2.9411764705882354e-05
  processed 94 batches
  training loss: 43.77665337920189
applying net to test data
  test loss: 22.569180250167847
  test accuracy: 0.7141253357263658
training epoch 35
  new learning rate: 2.857142857142857e-05
  processed 94 batches
  training loss: 43.71024665236473
applying net to test data
  test loss: 23.11243462562561
  test accuracy: 0.7069639467253918
training epoch 36
  new learning rate: 2.777777777777778e-05
  processed 94 batches
  training loss: 43.48720893263817
applying net to test data
  test loss: 22.671896755695343
  test accuracy: 0.7132336274470532
training epoch 37
  new learning rate: 2.7027027027027027e-05
  processed 94 batches
  training loss: 43.38790136575699
applying net to test data
  test loss: 22.6161008477211
  test accuracy: 0.7139227885536972
training epoch 38
  new learning rate: 2.6315789473684212e-05
  processed 94 batches
  training loss: 43.238596737384796
applying net to test data
  test loss: 22.68546360731125
  test accuracy: 0.7137074791383129
training epoch 39
  new learning rate: 2.5641025641025643e-05
  processed 94 batches
  training loss: 43.110884964466095
applying net to test data
  test loss: 22.654377043247223
  test accuracy: 0.7141618516170067
training epoch 40
  new learning rate: 2.5e-05
  processed 94 batches
  training loss: 42.91994571685791
applying net to test data
  test loss: 23.18229305744171
  test accuracy: 0.7082222794318078
training epoch 41
  new learning rate: 2.4390243902439026e-05
  processed 94 batches
  training loss: 42.70595940947533
applying net to test data
  test loss: 22.757486581802368
  test accuracy: 0.7130366370403517
training epoch 42
  new learning rate: 2.380952380952381e-05
  processed 94 batches
  training loss: 42.78268039226532
applying net to test data
  test loss: 22.282990992069244
  test accuracy: 0.7187962554969238
training epoch 43
  new learning rate: 2.3255813953488374e-05
  processed 94 batches
  training loss: 42.5568987429142
applying net to test data
  test loss: 22.752029836177826
  test accuracy: 0.7134360524929972
training epoch 44
  new learning rate: 2.272727272727273e-05
  processed 94 batches
  training loss: 42.473791748285294
applying net to test data
  test loss: 21.887010633945465
  test accuracy: 0.7237124057303813
training epoch 45
  new learning rate: 2.2222222222222223e-05
  processed 94 batches
  training loss: 42.34189063310623
applying net to test data
  test loss: 22.79641556739807
  test accuracy: 0.7133121549309398
training epoch 46
  new learning rate: 2.173913043478261e-05
  processed 94 batches
  training loss: 42.08870595693588
applying net to test data
  test loss: 22.80419260263443
  test accuracy: 0.7140162765613414
training epoch 47
  new learning rate: 2.1276595744680852e-05
  processed 94 batches
  training loss: 42.12477892637253
applying net to test data
  test loss: 22.718743562698364
  test accuracy: 0.7143930375065827
training epoch 48
  new learning rate: 2.0833333333333333e-05
  processed 94 batches
  training loss: 41.91080656647682
applying net to test data
  test loss: 22.30082356929779
  test accuracy: 0.7193620075484086
training epoch 49
  new learning rate: 2.0408163265306123e-05
  processed 94 batches
  training loss: 41.842140436172485
applying net to test data
  test loss: 22.37538105249405
  test accuracy: 0.7181811031853581
training epoch 50
  new learning rate: 2e-05
  processed 94 batches
  training loss: 41.723140984773636
applying net to test data
  test loss: 22.532807528972626
  test accuracy: 0.7170814741574966
grid_unet_28-06-2022_12:09, 50, 60, 3, 0.001->0.00001, AdamW, CrossEntropy ,  22.532807528972626, 0.7170814741574966, 3444.645665168762 

