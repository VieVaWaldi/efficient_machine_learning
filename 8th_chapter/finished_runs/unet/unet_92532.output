submit host:
fj087
submit dir:
/lustre/home/wehrenberger/code/efficient_machine_learning/8th_chapter
nodelist:
fj-epyc
##############################################
# Welcome to EML's U-Net for seismic example #
##############################################
CUDA devices: 2
   Tesla V100-PCIE-32GB
   Tesla V100-PCIE-32GB
printing configuration:
{
  "unet": {
    "n_init_channels": 45,
    "kernel_size": 3,
    "n_layers_per_block": 2,
    "n_levels": 5
  },
  "train": {
    "data": {
      "seismic": "data/data_train.npz",
      "labels": "data/labels_train.npz",
      "sample_shape": [
        1004,
        1,
        588
      ],
      "subset": [
        [
          0,
          1004
        ],
        [
          0,
          750
        ],
        [
          0,
          588
        ]
      ]
    },
    "n_epochs": 50,
    "n_epochs_print": 20,
    "n_batch_abort": 5000,
    "batch_size": 8
  },
  "test": {
    "data": {
      "seismic": "data/data_train.npz",
      "labels": "data/labels_train.npz",
      "sample_shape": [
        1004,
        1,
        588
      ],
      "subset": [
        [
          0,
          1004
        ],
        [
          751,
          782
        ],
        [
          0,
          588
        ]
      ]
    },
    "batch_size": 1
  }
}
********************
* assembling U-Net *
********************
encoder:
Sequential(
  (0): Conv2d(1, 45, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(45, 45, kernel_size=(3, 3), stride=(1, 1))
  (4): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
)
Sequential(
  (0): Conv2d(45, 90, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(90, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(90, 90, kernel_size=(3, 3), stride=(1, 1))
  (4): BatchNorm2d(90, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
)
Sequential(
  (0): Conv2d(90, 180, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1))
  (4): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
)
Sequential(
  (0): Conv2d(180, 360, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(360, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(360, 360, kernel_size=(3, 3), stride=(1, 1))
  (4): BatchNorm2d(360, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
)
max_pooling:
MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
crop:
ZeroPad2d((-88, -88, -88, -88))
ZeroPad2d((-40, -40, -40, -40))
ZeroPad2d((-16, -16, -16, -16))
ZeroPad2d((-4, -4, -4, -4))
bottleneck:
Sequential(
  (0): Conv2d(360, 360, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(360, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(360, 360, kernel_size=(3, 3), stride=(1, 1))
  (4): BatchNorm2d(360, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
)
up_sampling:
Upsample(scale_factor=2.0, mode=bilinear)
decoder:
Sequential(
  (0): Conv2d(90, 45, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(45, 45, kernel_size=(3, 3), stride=(1, 1))
  (4): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
)
Sequential(
  (0): Conv2d(180, 45, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(45, 45, kernel_size=(3, 3), stride=(1, 1))
  (4): BatchNorm2d(45, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
)
Sequential(
  (0): Conv2d(360, 90, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(90, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(90, 90, kernel_size=(3, 3), stride=(1, 1))
  (4): BatchNorm2d(90, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
)
Sequential(
  (0): Conv2d(720, 180, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(180, 180, kernel_size=(3, 3), stride=(1, 1))
  (4): BatchNorm2d(180, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
)
classification:
Conv2d(45, 8, kernel_size=(1, 1), stride=(1, 1))
*****************
* prepping data *
*****************
loading training dataset
shape padded: (1004, 750, 588)
shape: (1, 750, 1)
loading test data set
shape padded: (1004, 31, 588)
shape: (1, 31, 1)
deriving mean and standard deviation of training data
  mean: 0.6930369
  std: 388.38947
normalizing training and test data
initializing data loaders
************
* training *
************
training epoch 1
  processed 94 batches
  training loss: 187.28813898563385
applying net to test data
  test loss: 63.00461721420288
  test accuracy: 0.2822687756580536
training epoch 2
  processed 94 batches
  training loss: 187.08624708652496
applying net to test data
  test loss: 62.954240560531616
  test accuracy: 0.2833227520234321
training epoch 3
  processed 94 batches
  training loss: 186.90941274166107
applying net to test data
  test loss: 62.904438972473145
  test accuracy: 0.28443573704341324
training epoch 4
  processed 94 batches
  training loss: 186.7342723608017
applying net to test data
  test loss: 62.85252356529236
  test accuracy: 0.2856425906162607
training epoch 5
  processed 94 batches
  training loss: 186.5307002067566
applying net to test data
  test loss: 62.80021643638611
  test accuracy: 0.286840388405481
training epoch 6
  processed 94 batches
  training loss: 186.3544089794159
applying net to test data
  test loss: 62.74662256240845
  test accuracy: 0.2881418895233347
training epoch 7
  processed 94 batches
  training loss: 186.17888820171356
applying net to test data
  test loss: 62.6935760974884
  test accuracy: 0.2894261554400916
training epoch 8
  processed 94 batches
  training loss: 186.00386714935303
applying net to test data
  test loss: 62.63990592956543
  test accuracy: 0.29072716968785783
training epoch 9
  processed 94 batches
  training loss: 185.81639111042023
applying net to test data
  test loss: 62.584031105041504
  test accuracy: 0.29216441018610123
training epoch 10
  processed 94 batches
  training loss: 185.64902687072754
applying net to test data
  test loss: 62.52936315536499
  test accuracy: 0.293552282057474
training epoch 11
  processed 94 batches
  training loss: 185.45955801010132
applying net to test data
  test loss: 62.474050760269165
  test accuracy: 0.2949903015478574
training epoch 12
  processed 94 batches
  training loss: 185.2789307832718
applying net to test data
  test loss: 62.41655683517456
  test accuracy: 0.29646084396008443
training epoch 13
  processed 94 batches
  training loss: 185.0943487882614
applying net to test data
  test loss: 62.359779834747314
  test accuracy: 0.2979736466959048
training epoch 14
  processed 94 batches
  training loss: 184.9113825559616
applying net to test data
  test loss: 62.30051922798157
  test accuracy: 0.2995271517710386
training epoch 15
  processed 94 batches
  training loss: 184.73761022090912
applying net to test data
  test loss: 62.242308139801025
  test accuracy: 0.30106634286560047
training epoch 16
  processed 94 batches
  training loss: 184.5450975894928
applying net to test data
  test loss: 62.1831328868866
  test accuracy: 0.3026343566693412
training epoch 17
  processed 94 batches
  training loss: 184.35074126720428
applying net to test data
  test loss: 62.12403154373169
  test accuracy: 0.30419662540604964
training epoch 18
  processed 94 batches
  training loss: 184.17291486263275
applying net to test data
  test loss: 62.06356763839722
  test accuracy: 0.30581176823425854
training epoch 19
  processed 94 batches
  training loss: 183.97887229919434
applying net to test data
  test loss: 62.001665353775024
  test accuracy: 0.30748397223672014
training epoch 20
  processed 94 batches
  training loss: 183.782519698143
applying net to test data
  test loss: 61.93988215923309
  test accuracy: 0.3091441992350297
training epoch 21
  processed 94 batches
  training loss: 183.5926789045334
applying net to test data
  test loss: 61.87936353683472
  test accuracy: 0.3107926439772223
training epoch 22
  processed 94 batches
  training loss: 183.3971140384674
applying net to test data
  test loss: 61.814972281455994
  test accuracy: 0.31251246387423953
training epoch 23
  processed 94 batches
  training loss: 183.20354664325714
applying net to test data
  test loss: 61.75349700450897
  test accuracy: 0.3142427027911288
training epoch 24
  processed 94 batches
  training loss: 183.00909757614136
applying net to test data
  test loss: 61.689645886421204
  test accuracy: 0.3159697283654408
training epoch 25
  processed 94 batches
  training loss: 182.80622494220734
applying net to test data
  test loss: 61.62604022026062
  test accuracy: 0.3176703655810113
training epoch 26
  processed 94 batches
  training loss: 182.6055999994278
applying net to test data
  test loss: 61.56286942958832
  test accuracy: 0.3194210530415748
training epoch 27
  processed 94 batches
  training loss: 182.41855943202972
applying net to test data
  test loss: 61.49875342845917
  test accuracy: 0.3211935522820575
training epoch 28
  processed 94 batches
  training loss: 182.215705037117
applying net to test data
  test loss: 61.43410670757294
  test accuracy: 0.32292749141161164
training epoch 29
  processed 94 batches
  training loss: 182.01234889030457
applying net to test data
  test loss: 61.368682980537415
  test accuracy: 0.32469940640798933
training epoch 30
  processed 94 batches
  training loss: 181.80788052082062
applying net to test data
  test loss: 61.302531719207764
  test accuracy: 0.32647881920371424
training epoch 31
  processed 94 batches
  training loss: 181.59664940834045
applying net to test data
  test loss: 61.23773968219757
  test accuracy: 0.3282411915463773
training epoch 32
  processed 94 batches
  training loss: 181.3997005224228
applying net to test data
  test loss: 61.1723872423172
  test accuracy: 0.3300111590624051
training epoch 33
  processed 94 batches
  training loss: 181.18599605560303
applying net to test data
  test loss: 61.1076465845108
  test accuracy: 0.3317249417703375
training epoch 34
  processed 94 batches
  training loss: 180.98322093486786
applying net to test data
  test loss: 61.0415495634079
  test accuracy: 0.33348351652631825
training epoch 35
  processed 94 batches
  training loss: 180.7866724729538
applying net to test data
  test loss: 60.97365665435791
  test accuracy: 0.3352707192434428
training epoch 36
  processed 94 batches
  training loss: 180.57018637657166
applying net to test data
  test loss: 60.90895223617554
  test accuracy: 0.33698859166011014
training epoch 37
  processed 94 batches
  training loss: 180.3720246553421
applying net to test data
  test loss: 60.840938448905945
  test accuracy: 0.3387968271650139
training epoch 38
  processed 94 batches
  training loss: 180.16979277133942
applying net to test data
  test loss: 60.77623796463013
  test accuracy: 0.3405179129242586
training epoch 39
  processed 94 batches
  training loss: 179.96122586727142
applying net to test data
  test loss: 60.70843482017517
  test accuracy: 0.3422283849155962
training epoch 40
  processed 94 batches
  training loss: 179.75437271595
applying net to test data
  test loss: 60.640928506851196
  test accuracy: 0.34399367847878415
training epoch 41
  processed 94 batches
  training loss: 179.54595613479614
applying net to test data
  test loss: 60.575761556625366
  test accuracy: 0.3456390072524168
training epoch 42
  processed 94 batches
  training loss: 179.34839284420013
applying net to test data
  test loss: 60.50654625892639
  test accuracy: 0.34737966518917823
training epoch 43
  processed 94 batches
  training loss: 179.13165509700775
applying net to test data
  test loss: 60.439592242240906
  test accuracy: 0.34909354527112824
training epoch 44
  processed 94 batches
  training loss: 178.93159544467926
applying net to test data
  test loss: 60.37237536907196
  test accuracy: 0.35075007205677294
training epoch 45
  processed 94 batches
  training loss: 178.72347569465637
applying net to test data
  test loss: 60.306387066841125
  test accuracy: 0.35237933411751876
training epoch 46
  processed 94 batches
  training loss: 178.51105415821075
applying net to test data
  test loss: 60.23687732219696
  test accuracy: 0.35411035202654806
training epoch 47
  processed 94 batches
  training loss: 178.31650149822235
applying net to test data
  test loss: 60.171849489212036
  test accuracy: 0.3557060200512577
training epoch 48
  processed 94 batches
  training loss: 178.10052156448364
applying net to test data
  test loss: 60.103649497032166
  test accuracy: 0.35733615847816097
training epoch 49
  processed 94 batches
  training loss: 177.89843380451202
applying net to test data
  test loss: 60.03582274913788
  test accuracy: 0.3589786634052862
training epoch 50
  processed 94 batches
  training loss: 177.68862736225128
applying net to test data
  test loss: 59.96767830848694
  test accuracy: 0.36053431070880493
grid_unet_26-06-2022_20:19, 50, 45, 5, 0.0001, Adadelta, CrossEntropy ,  59.96767830848694, 0.36053431070880493, 2828.9231390953064 

