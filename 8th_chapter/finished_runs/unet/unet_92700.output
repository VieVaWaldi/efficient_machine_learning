submit host:
fj081
submit dir:
/lustre/home/wehrenberger/code/efficient_machine_learning/8th_chapter
nodelist:
fj-epyc
##############################################
# Welcome to EML's U-Net for seismic example #
##############################################
CUDA devices: 2
   Tesla V100-PCIE-32GB
   Tesla V100-PCIE-32GB
printing configuration:
{
  "unet": {
    "n_init_channels": 60,
    "kernel_size": 3,
    "n_layers_per_block": 2,
    "n_levels": 4
  },
  "train": {
    "data": {
      "seismic": "data/data_train.npz",
      "labels": "data/labels_train.npz",
      "sample_shape": [
        1004,
        1,
        588
      ],
      "subset": [
        [
          0,
          1004
        ],
        [
          0,
          750
        ],
        [
          0,
          588
        ]
      ]
    },
    "n_epochs": 50,
    "n_epochs_print": 20,
    "n_batch_abort": 5000,
    "batch_size": 8
  },
  "test": {
    "data": {
      "seismic": "data/data_train.npz",
      "labels": "data/labels_train.npz",
      "sample_shape": [
        1004,
        1,
        588
      ],
      "subset": [
        [
          0,
          1004
        ],
        [
          751,
          782
        ],
        [
          0,
          588
        ]
      ]
    },
    "batch_size": 1
  }
}
********************
* assembling U-Net *
********************
encoder:
Sequential(
  (0): Conv2d(1, 60, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1))
  (4): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
)
Sequential(
  (0): Conv2d(60, 120, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1))
  (4): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
)
Sequential(
  (0): Conv2d(120, 240, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1))
  (4): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
)
max_pooling:
MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
crop:
ZeroPad2d((-40, -40, -40, -40))
ZeroPad2d((-16, -16, -16, -16))
ZeroPad2d((-4, -4, -4, -4))
bottleneck:
Sequential(
  (0): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(240, 240, kernel_size=(3, 3), stride=(1, 1))
  (4): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
)
up_sampling:
Upsample(scale_factor=2.0, mode=bilinear)
decoder:
Sequential(
  (0): Conv2d(120, 60, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1))
  (4): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
)
Sequential(
  (0): Conv2d(240, 60, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(60, 60, kernel_size=(3, 3), stride=(1, 1))
  (4): BatchNorm2d(60, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
)
Sequential(
  (0): Conv2d(480, 120, kernel_size=(3, 3), stride=(1, 1))
  (1): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (2): ReLU(inplace=True)
  (3): Conv2d(120, 120, kernel_size=(3, 3), stride=(1, 1))
  (4): BatchNorm2d(120, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (5): ReLU(inplace=True)
)
classification:
Conv2d(60, 8, kernel_size=(1, 1), stride=(1, 1))
*****************
* prepping data *
*****************
loading training dataset
shape padded: (1004, 750, 588)
shape: (1, 750, 1)
loading test data set
shape padded: (1004, 31, 588)
shape: (1, 31, 1)
deriving mean and standard deviation of training data
  mean: 0.6930369
  std: 388.38947
normalizing training and test data
initializing data loaders
************
* training *
************
training epoch 1
  new learning rate: 0.001
  processed 94 batches
  training loss: 128.1362726688385
applying net to test data
  test loss: 33.05313181877136
  test accuracy: 0.6469128750528244
training epoch 2
  new learning rate: 0.0005
  processed 94 batches
  training loss: 80.21687430143356
applying net to test data
  test loss: 27.397938549518585
  test accuracy: 0.6998351880546556
training epoch 3
  new learning rate: 0.0003333333333333333
  processed 94 batches
  training loss: 63.47678965330124
applying net to test data
  test loss: 24.630766689777374
  test accuracy: 0.7263293421608678
training epoch 4
  new learning rate: 0.00025
  processed 94 batches
  training loss: 53.912449181079865
applying net to test data
  test loss: 22.411457359790802
  test accuracy: 0.7544130863501902
training epoch 5
  new learning rate: 0.0002
  processed 94 batches
  training loss: 47.495531529188156
applying net to test data
  test loss: 21.386986255645752
  test accuracy: 0.7619069587265812
training epoch 6
  new learning rate: 0.00016666666666666666
  processed 94 batches
  training loss: 43.12277874350548
applying net to test data
  test loss: 20.808141231536865
  test accuracy: 0.7680454993661079
training epoch 7
  new learning rate: 0.00014285714285714287
  processed 94 batches
  training loss: 39.63201454281807
applying net to test data
  test loss: 20.109808206558228
  test accuracy: 0.7771903084941542
training epoch 8
  new learning rate: 0.000125
  processed 94 batches
  training loss: 36.81577858328819
applying net to test data
  test loss: 19.77777749300003
  test accuracy: 0.7798777292576419
training epoch 9
  new learning rate: 0.00011111111111111112
  processed 94 batches
  training loss: 34.888878136873245
applying net to test data
  test loss: 19.41889727115631
  test accuracy: 0.7835821242428511
training epoch 10
  new learning rate: 0.0001
  processed 94 batches
  training loss: 32.94178143143654
applying net to test data
  test loss: 19.025392472743988
  test accuracy: 0.786826877024933
training epoch 11
  new learning rate: 9.090909090909092e-05
  processed 94 batches
  training loss: 31.307960212230682
applying net to test data
  test loss: 18.555264115333557
  test accuracy: 0.7933387096774194
training epoch 12
  new learning rate: 8.333333333333333e-05
  processed 94 batches
  training loss: 30.008889943361282
applying net to test data
  test loss: 18.419887125492096
  test accuracy: 0.7951837582758134
training epoch 13
  new learning rate: 7.692307692307693e-05
  processed 94 batches
  training loss: 28.767796486616135
applying net to test data
  test loss: 17.677628755569458
  test accuracy: 0.8038120862093252
training epoch 14
  new learning rate: 7.142857142857143e-05
  processed 94 batches
  training loss: 27.815591245889664
applying net to test data
  test loss: 18.646751761436462
  test accuracy: 0.792232497534864
training epoch 15
  new learning rate: 6.666666666666667e-05
  processed 94 batches
  training loss: 26.82606515288353
applying net to test data
  test loss: 17.797094851732254
  test accuracy: 0.8019714044231582
training epoch 16
  new learning rate: 6.25e-05
  processed 94 batches
  training loss: 26.07367503643036
applying net to test data
  test loss: 17.48117220401764
  test accuracy: 0.8058455416255811
training epoch 17
  new learning rate: 5.882352941176471e-05
  processed 94 batches
  training loss: 25.224054157733917
applying net to test data
  test loss: 17.308431446552277
  test accuracy: 0.8077045358501197
training epoch 18
  new learning rate: 5.555555555555556e-05
  processed 94 batches
  training loss: 24.65471450984478
applying net to test data
  test loss: 17.886807799339294
  test accuracy: 0.8003222284828849
training epoch 19
  new learning rate: 5.2631578947368424e-05
  processed 94 batches
  training loss: 24.046945244073868
applying net to test data
  test loss: 17.1091088950634
  test accuracy: 0.8113039864769686
training epoch 20
  new learning rate: 5e-05
  processed 94 batches
  training loss: 23.620677158236504
applying net to test data
  test loss: 17.25674879550934
  test accuracy: 0.8091191012818707
training epoch 21
  new learning rate: 4.761904761904762e-05
  processed 94 batches
  training loss: 23.022096380591393
applying net to test data
  test loss: 17.509550988674164
  test accuracy: 0.8066083955486688
training epoch 22
  new learning rate: 4.545454545454546e-05
  processed 94 batches
  training loss: 22.44511817395687
applying net to test data
  test loss: 17.0577053129673
  test accuracy: 0.8118982955345824
training epoch 23
  new learning rate: 4.347826086956522e-05
  processed 94 batches
  training loss: 22.09882040321827
applying net to test data
  test loss: 16.93070462346077
  test accuracy: 0.8131814340047894
training epoch 24
  new learning rate: 4.1666666666666665e-05
  processed 94 batches
  training loss: 21.67978572845459
applying net to test data
  test loss: 16.9971342086792
  test accuracy: 0.8131180447950416
training epoch 25
  new learning rate: 4e-05
  processed 94 batches
  training loss: 21.32414124906063
applying net to test data
  test loss: 18.05742710828781
  test accuracy: 0.8014027327792647
training epoch 26
  new learning rate: 3.846153846153846e-05
  processed 94 batches
  training loss: 20.913720786571503
applying net to test data
  test loss: 17.288218915462494
  test accuracy: 0.8108111705874067
training epoch 27
  new learning rate: 3.7037037037037037e-05
  processed 94 batches
  training loss: 20.532485276460648
applying net to test data
  test loss: 17.02659198641777
  test accuracy: 0.8135990280321171
training epoch 28
  new learning rate: 3.571428571428572e-05
  processed 94 batches
  training loss: 20.239329367876053
applying net to test data
  test loss: 17.080427527427673
  test accuracy: 0.8120619805606424
training epoch 29
  new learning rate: 3.4482758620689657e-05
  processed 94 batches
  training loss: 19.996363267302513
applying net to test data
  test loss: 17.093502640724182
  test accuracy: 0.8121233272291872
training epoch 30
  new learning rate: 3.3333333333333335e-05
  processed 94 batches
  training loss: 19.666103661060333
applying net to test data
  test loss: 16.65881460905075
  test accuracy: 0.8181663614593605
training epoch 31
  new learning rate: 3.2258064516129034e-05
  processed 94 batches
  training loss: 19.39153443276882
applying net to test data
  test loss: 16.717327624559402
  test accuracy: 0.8172045358501198
training epoch 32
  new learning rate: 3.125e-05
  processed 94 batches
  training loss: 19.203014820814133
applying net to test data
  test loss: 16.85651931166649
  test accuracy: 0.816214044231582
training epoch 33
  new learning rate: 3.0303030303030302e-05
  processed 94 batches
  training loss: 18.881179437041283
applying net to test data
  test loss: 16.573512017726898
  test accuracy: 0.8198926609381603
training epoch 34
  new learning rate: 2.9411764705882354e-05
  processed 94 batches
  training loss: 18.71120572090149
applying net to test data
  test loss: 16.6961932182312
  test accuracy: 0.8177581349485843
training epoch 35
  new learning rate: 2.857142857142857e-05
  processed 94 batches
  training loss: 18.358176678419113
applying net to test data
  test loss: 16.828145772218704
  test accuracy: 0.8169523172277786
training epoch 36
  new learning rate: 2.777777777777778e-05
  processed 94 batches
  training loss: 18.264179572463036
applying net to test data
  test loss: 16.72862395644188
  test accuracy: 0.8180717002394704
training epoch 37
  new learning rate: 2.7027027027027027e-05
  processed 94 batches
  training loss: 18.07625798881054
applying net to test data
  test loss: 16.713646471500397
  test accuracy: 0.8183523031412875
training epoch 38
  new learning rate: 2.6315789473684212e-05
  processed 94 batches
  training loss: 17.778427988290787
applying net to test data
  test loss: 16.46770602464676
  test accuracy: 0.8216561487533456
training epoch 39
  new learning rate: 2.5641025641025643e-05
  processed 94 batches
  training loss: 17.672427341341972
applying net to test data
  test loss: 16.085476219654083
  test accuracy: 0.8251327651781941
training epoch 40
  new learning rate: 2.5e-05
  processed 94 batches
  training loss: 17.618236184120178
applying net to test data
  test loss: 16.252364933490753
  test accuracy: 0.8237685589519651
training epoch 41
  new learning rate: 2.4390243902439026e-05
  processed 94 batches
  training loss: 17.306139662861824
applying net to test data
  test loss: 16.39478263258934
  test accuracy: 0.822656007888435
training epoch 42
  new learning rate: 2.380952380952381e-05
  processed 94 batches
  training loss: 17.183541744947433
applying net to test data
  test loss: 16.922754168510437
  test accuracy: 0.8167918720946612
training epoch 43
  new learning rate: 2.3255813953488374e-05
  processed 94 batches
  training loss: 16.942787542939186
applying net to test data
  test loss: 16.45004865527153
  test accuracy: 0.8229026623468094
training epoch 44
  new learning rate: 2.272727272727273e-05
  processed 94 batches
  training loss: 16.93160080909729
applying net to test data
  test loss: 15.902037769556046
  test accuracy: 0.8282117199605579
training epoch 45
  new learning rate: 2.2222222222222223e-05
  processed 94 batches
  training loss: 16.626268655061722
applying net to test data
  test loss: 16.546990633010864
  test accuracy: 0.8214621073390619
training epoch 46
  new learning rate: 2.173913043478261e-05
  processed 94 batches
  training loss: 16.63872952759266
applying net to test data
  test loss: 16.18622037768364
  test accuracy: 0.8258422313001831
training epoch 47
  new learning rate: 2.1276595744680852e-05
  processed 94 batches
  training loss: 16.346433341503143
applying net to test data
  test loss: 16.512112140655518
  test accuracy: 0.8220235948725173
training epoch 48
  new learning rate: 2.0833333333333333e-05
  processed 94 batches
  training loss: 16.22350510954857
applying net to test data
  test loss: 16.372802942991257
  test accuracy: 0.8238790674742922
training epoch 49
  new learning rate: 2.0408163265306123e-05
slurmstepd: error: *** JOB 92700 ON fj-epyc CANCELLED AT 2022-06-28T14:27:50 DUE TO TIME LIMIT ***
